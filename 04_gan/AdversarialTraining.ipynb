{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "Original Paper: https://arxiv.org/abs/1406.2661\n",
    "\n",
    "Given a generative function $G$, the value of the discriminative function $D$ can be defined as:\n",
    "\n",
    "$$\n",
    "V(D, G) \n",
    "= \\mathop{\\mathbb{E}}_{\\textbf{x}} \\big[\\ \\text{log}\\ D(\\textbf{x})\\ \\big]\n",
    "+ \\mathop{\\mathbb{E}}_{\\textbf{z}} \\big[\\ \\text{log}\\ (1 - D(G(\\textbf{z})))\\ \\big]\n",
    "$$\n",
    "\n",
    "In this equation, $\\textbf{x} \\sim  p_{\\text{data}}(\\textbf{x})$ is a random sample from the training data,\n",
    "while $\\textbf{z} \\sim \\text{U}$ is a random sample from the uniform distribution.\n",
    "The first term in the sum rewards positives response for instances from the training data, while\n",
    "the second term rewards negative responses for samples from the generator.\n",
    "\n",
    "During training the models are updated one at a time. The cost of the discriminator is calculated on a batch of $m$ pairs of $(\\textbf{x}^{(i)}, \\textbf{z}^{(i)})$ as follows:\n",
    "\n",
    "$$\n",
    "C_D\n",
    "= -\\frac{1}{m} \n",
    "\\sum_{i=1}^m\n",
    "{\\big[\\ \n",
    "    \\text{log}\\ D(\\textbf{x}^{(i)})\n",
    "    + \\text{log}\\ (1 - D(G(\\textbf{z}^{(i)})))\\ \n",
    "\\big]\\ \n",
    "}\n",
    "$$\n",
    "\n",
    "The generator is trained on batch of $m$ samples of $(\\textbf{z}^{(i)})$ with the following cost:\n",
    "$$\n",
    "C_G\n",
    "= -\\frac{1}{m} \n",
    "\\sum_{i=1}^m\n",
    "{\\big[\\ \n",
    "    \\text{log}\\ (D(G(\\textbf{z}^{(i)})))\\ \n",
    "\\big]\\ \n",
    "}\n",
    "$$\n",
    "\n",
    "Note that the generator does not look at the training data at all. Instead it learns to map $(\\textbf{z}^{(i)})$ into a distribution that looks similar to $p_{\\text{data}}(\\textbf{x})$ by maximizing the discriminator's probability to respond positively to the generator's output $G(\\textbf{z}^{(i)})$. The term inside of the log is not the same as the false negative cost for the discriminator. This adjustment increases the magnitude of the gradient when the generator's samples are bad (e.g. $D$ is almost always 0).\n",
    "\n",
    "Combined training is the minimax optimization that converges on the training data distribution:\n",
    "\n",
    "$$ \\min_G\\max_D V(D,G) $$\n",
    "\n",
    "$$ G(\\textbf{z}) \\rightarrow p_{\\text{data}}(\\textbf{x}) $$\n",
    "\n",
    "$$ D(G(\\textbf{z})) \\rightarrow 0.5 $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
